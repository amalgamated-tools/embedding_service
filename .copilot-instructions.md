# Copilot Instructions

## Repository Overview

This is an **Embedding Service** - a FastAPI-based web service that provides text embedding and classification capabilities using machine learning models.

### Purpose
- Generate text embeddings using sentence-transformers models
- Classify text into predefined categories (Engineering/Non-Engineering)
- Provide batch processing capabilities for multiple texts
- Serve as a microservice for ML-powered text analysis

## Technology Stack

### Core Technologies
- **Python 3.13**: Primary programming language
- **FastAPI**: Modern web framework for building APIs
- **sentence-transformers**: HuggingFace library for text embeddings
- **numpy**: Numerical computations
- **uvicorn**: ASGI server for running the application
- **Docker**: Containerization for deployment

### Development Tools
- **uv**: Modern Python package manager (preferred over pip)
- **pyproject.toml**: Project configuration and dependencies

## Application Architecture

### Main Components
- `main.py`: Single-file FastAPI application containing all endpoints
- Model: Uses `all-MiniLM-L6-v2` sentence transformer model
- Caching: LRU cache for embedding computations (maxsize=5000)

### API Endpoints

#### GET /health
- **Purpose**: Health check endpoint for monitoring and load balancing
- **Input**: None
- **Output**: `{"status": "healthy", "service": "embedding-service"}`
- **Use Case**: Verify service availability and monitor uptime

#### POST /embed
- **Purpose**: Generate embedding vector for a single text
- **Input**: `{"text": "your text here"}`
- **Output**: `{"embedding": [0.1, 0.2, ...]}`
- **Use Case**: Get vector representation of text for similarity search

#### POST /classify  
- **Purpose**: Classify text into Engineering/Non-Engineering categories
- **Input**: `{"text": "your text here"}`
- **Output**: `{"category": "Engineering", "similarity": 0.85}`
- **Use Case**: Categorize text content automatically

#### POST /classify_batch
- **Purpose**: Classify multiple texts in a single request
- **Input**: `{"texts": ["text1", "text2", ...]}`
- **Output**: `{"results": [{"text": "text1", "category": "Engineering", "similarity": 0.85}, ...]}`
- **Use Case**: Efficient batch processing of multiple texts

## Development Workflow

### Setup
1. Use `uv sync` to install dependencies (preferred)
2. Alternative: `pip install fastapi numpy sentence-transformers uvicorn`
3. Run locally: `python main.py` or `uvicorn main:app --reload`
4. Access API docs at: `http://localhost:8000/docs`

### Code Style and Patterns

#### Function Definitions
- Use type hints for all function parameters and return types
- Follow FastAPI patterns for endpoint definitions
- Use Pydantic models for request/response schemas

#### Example Pydantic Models
```python
class Item(BaseModel):
    text: str

class Items(BaseModel):
    texts: List[str]
```

#### Caching Pattern
- Use `@lru_cache(maxsize=5000)` for expensive operations
- Cache embedding computations to improve performance
- Example: `cached_embed(text: str)` function

#### Error Handling
- Use FastAPI's automatic validation for request bodies
- Return appropriate HTTP status codes
- Provide clear error messages

### Model Loading
- Load models once at startup (global variable)
- Pre-encode anchor categories for classification
- Use normalized embeddings for consistency

### Performance Considerations
- Embeddings are cached using LRU cache
- Batch processing reduces overhead for multiple texts
- Model is loaded once at startup to avoid repeated loading

## Docker Deployment

### Container Configuration
- Base image: `ghcr.io/astral-sh/uv:python3.12-bookworm-slim`
- Pre-downloads model during build time
- Exposes port 8000
- Uses multi-stage build for optimization

### Build and Run
```bash
docker build -t embedding-service .
docker run -p 8000:8000 embedding-service
```

## Testing

### Manual Testing
- Use FastAPI's automatic documentation at `/docs`
- Test endpoints with curl or HTTP clients
- Verify embedding dimensions and classification accuracy

### Example Test Cases
```python
# Test embedding endpoint
response = client.post("/embed", json={"text": "Machine learning model"})
assert "embedding" in response.json()

# Test classification
response = client.post("/classify", json={"text": "Python programming"})
assert response.json()["category"] in ["Engineering", "Non-Engineering"]
```

## Common Patterns and Conventions

### When Adding New Endpoints
1. Define Pydantic model for request/response
2. Add type hints to function signature
3. Use FastAPI decorators (@app.post, @app.get, etc.)
4. Include docstring describing purpose
5. Consider caching if operation is expensive

### When Modifying Models
1. Update model loading in global scope
2. Consider backward compatibility
3. Update Docker build process if needed
4. Test with existing cached embeddings

### Code Organization
- Keep single-file structure for simplicity
- Group related imports together
- Place model loading after imports, before endpoint definitions
- Define Pydantic models before endpoints that use them

## Deployment Notes

### Environment Variables
- Consider adding configurable model names
- Add port configuration
- Include logging level settings

### Monitoring
- Add health check endpoint
- Consider metrics for request counts and response times
- Monitor memory usage (models can be large)

### Scaling Considerations
- Service is stateless except for model loading
- Can be horizontally scaled
- Consider model caching strategies for multiple instances

## Security Considerations

### Input Validation
- Pydantic automatically validates request schemas
- Consider text length limits to prevent abuse
- Rate limiting may be needed for production

### Model Security
- Models are loaded from trusted sources (HuggingFace)
- No user-provided model loading
- Consider input sanitization for sensitive deployments